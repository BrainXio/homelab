name: ollama

x-ollama-base: &ollama-base
  image: docker.io/ollama/ollama:latest
  pull_policy: always
  tty: true
  restart: unless-stopped
  env_file:
    - .env
  volumes:
    - model_data:/root/.ollama
  healthcheck:
    test: ["CMD", "ollama", "ps"]
    interval: 10s
    timeout: 5s
    retries: 5
    start_period: 10s
    start_interval: 2s
  labels:
    - com.docker.compose.project=ollama
    - app.name=ollama

x-tailscale-base: &tailscale-base
  image: tailscale/tailscale:latest
  environment:
    - TS_EXTRA_ARGS=--auth-key file:/run/secrets/tsauthkey --accept-dns --hostname ${ENV_TYPE}ollama${ENV_SUFFIX}
    - TS_STATE_DIR=/var/lib/tailscale
    - TS_USERSPACE=false
  volumes:
    - tun_state:/var/lib/tailscale
  devices:
    - /dev/net/tun:/dev/net/tun
  cap_add:
    - NET_ADMIN
  restart: unless-stopped
  secrets:
    - tsauthkey
  healthcheck:
    test: ["CMD", "tailscale", "status"]
    interval: 10s
    timeout: 5s
    retries: 3
    start_period: 10s
    start_interval: 2s
  labels:
    - com.docker.compose.project=ollama
    - app.env=${ENV_TYPE}
    - app.name=tailscale

services:
  dev-loc-cpu:
    <<: *ollama-base
    ports:
    - ${OLLAMA_PORT:-11435}:11434    
    runtime: runc
    profiles:
      - loc-cpu
    labels:
      - com.docker.compose.project=ollama
      - app.name=ollama
      - app.mode=loc-cpu

  dev-loc-gpu:
    <<: *ollama-base
    ports:
    - ${OLLAMA_PORT:-11435}:11434
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    profiles:
      - loc-gpu
    labels:
      - com.docker.compose.project=ollama
      - app.name=ollama
      - app.mode=loc-gpu

  tail-cpu:
    <<: *ollama-base
    network_mode: service:tail-tun
    depends_on:
      tail-tun:
        condition: service_healthy
    runtime: runc
    profiles:
      - tail-cpu
    labels:
      - com.docker.compose.project=ollama
      - app.name=ollama
      - app.mode=${ENV_TYPE}tail-cpu

  tail-gpu:
    <<: *ollama-base
    network_mode: service:tail-tun
    depends_on:
      tail-tun:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    profiles:
      - tail-gpu
    labels:
      - com.docker.compose.project=ollama
      - app.name=ollama
      - app.mode=${ENV_TYPE}tail-gpu

  tail-tun:
    <<: *tailscale-base
    profiles:
      - tail-cpu
      - tail-gpu
    labels:
      - com.docker.compose.project=ollama
      - app.name=tailscale

secrets:
  tsauthkey:
    file: ${TSAUTHKEY_PATH:-./tsauthkey.key}
    external: ${TSAUTHKEY_EXTERNAL:-false}

volumes:
  model_data:
  tun_state: