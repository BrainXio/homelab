# Deployment Settings
# Environment type (e.g., dev-, prod-, 8gb-)
ENV_TYPE=
# Environment suffix (e.g., -0, -gpu)
ENV_SUFFIX=
# Path to Tailscale auth key file
TSAUTHKEY_PATH=~/.secrets/ollama-tsauthkey.key
# Set to true if tsauthkey is managed externally
TSAUTHKEY_EXTERNAL=false

# Ollama Instance Settings
# Port for Ollama service (default: 11434)
OLLAMA_PORT=11434
# Duration to keep models loaded in memory (e.g., 5m, 10m)
# OLLAMA_KEEP_ALIVE=10m
# Enable flash attention for GPU acceleration (1 to enable, 0 to disable)
# OLLAMA_FLASH_ATTENTION=1
# Key-value cache quantization type (e.g., q8_0, fp16)
# OLLAMA_KV_CACHE_TYPE=q8_0
# Context window size for models (e.g., 4096, 8192)
# OLLAMA_NUM_CTX=8192
# Maximum number of models loaded simultaneously
# OLLAMA_MAX_LOADED_MODELS=1

# LLM Model Settings
# Default model for chat tasks
# LLM_CHAT_MODEL=granite3.3:8b
# Default model for text generation
# LLM_GENERATE_MODEL=granite3.3:8b
# Default model for embeddings
# LLM_EMBEDDING_MODEL=granite3.3:8b
# Default model for tool usage
# LLM_TOOLS_MODEL=granite3.3:8b